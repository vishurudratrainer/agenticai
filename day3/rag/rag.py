from langchain_ollama import ChatOllama
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_community.vectorstores import FAISS
from langchain_ollama import OllamaEmbeddings
from langchain_core.documents import Document
"""
* **`FAISS` (Facebook AI Similarity Search)**: 
This is an efficient
, **in-memory vector store** used in the examples. 
It stores the embeddings generated by `OllamaEmbeddings` and provides extremely fast similarity search, returning the most relevant document chunks based on the user's question.
"""
# 1. Define Documents (Simple in-memory store for demo)
docs = [
    Document(page_content="The new LangChain version uses LCEL (LangChain Expression Language) for building sequences."),
    Document(page_content="Ollama allows running large language models locally on your machine."),
    Document(page_content="The key components of an Agent are the LLM, Tools, and the Agent Executor (Loop)."),
]

# 2. Create Embeddings and Vector Store
# Use Ollama for embeddings to keep everything local
embeddings = OllamaEmbeddings(model="llama3") 
vectorstore = FAISS.from_documents(docs, embeddings)
retriever = vectorstore.as_retriever()

# 3. Define the Prompt
template = """
You are a helpful assistant. Answer the question based ONLY on the following context:
{context}

Question: {question}
"""
"""
RAG is essential for grounding the LLM's answer in specific documents or data, preventing hallucinations. This chain uses a simple in-memory vector store."""
prompt = ChatPromptTemplate.from_template(template)
ollama_model = ChatOllama(model="llama3", temperature=0)

# 4. Create the RAG Chain
rag_chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | prompt
    | ollama_model
    | StrOutputParser()
)

print("\n--- RAG Chain Result ---")
# The question "What does LCEL stand for?" will be answered using the retrieved document context.
#rag_result = rag_chain.invoke("What is LCEL used for?")
rag_result = rag_chain.invoke("What is the use of Ollamma ?")
print(rag_result)
#